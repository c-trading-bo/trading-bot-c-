name: "üö® Quality Assurance Monitor"

on:
  schedule:
    - cron: '0 */2 * * *'   # Every 2 hours
  workflow_dispatch:        # Manual trigger
  workflow_run:
    workflows: ["24/7 Continuous ML/RL Trainer (Compliant)"]
    types: [completed]

permissions:
  contents: write
  pull-requests: write
  actions: read

jobs:
  quality-monitor:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: "üì• Checkout Code"
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true
          fetch-depth: 0
        
      - name: "üêç Setup Python"
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          
      - name: "üì¶ Install Dependencies"
        run: |
          pip install --retry-delays 1,2,3 --timeout 60 requests pandas numpy scikit-learn
          pip install --retry-delays 1,2,3 --timeout 60 awscli boto3
          
      - name: "üîç Quality Assessment"
        run: |
          # Create quality monitoring script
          cat > quality_monitor.py << 'EOF'
          import json
          import requests
          import datetime
          import os
          from pathlib import Path
          import pandas as pd
          
          def check_workflow_quality():
              """Assess training workflow quality"""
              try:
                  headers = {
                      'Accept': 'application/vnd.github.v3+json',
                      'Authorization': f'token ${{ secrets.GITHUB_TOKEN }}'
                  }
                  
                  repo = '${{ github.repository }}'
                  
                  # Get recent training runs
                  url = f'https://api.github.com/repos/{repo}/actions/workflows/train-continuous-clean.yml/runs'
                  response = requests.get(url, headers=headers, params={'per_page': 20})
                  
                  if response.status_code != 200:
                      return {'error': f'Failed to fetch runs: {response.status_code}'}
                  
                  runs = response.json().get('workflow_runs', [])
                  
                  # Calculate quality metrics
                  total_runs = len(runs)
                  successful_runs = len([r for r in runs if r['conclusion'] == 'success'])
                  failed_runs = len([r for r in runs if r['conclusion'] == 'failure'])
                  
                  if total_runs == 0:
                      return {'error': 'No workflow runs found'}
                  
                  success_rate = (successful_runs / total_runs) * 100
                  
                  # Check for consecutive failures (quality alert)
                  consecutive_failures = 0
                  for run in runs:
                      if run['conclusion'] == 'failure':
                          consecutive_failures += 1
                      else:
                          break
                  
                  # Check training frequency
                  if len(runs) >= 2:
                      latest_time = datetime.datetime.fromisoformat(runs[0]['created_at'].replace('Z', '+00:00'))
                      previous_time = datetime.datetime.fromisoformat(runs[1]['created_at'].replace('Z', '+00:00'))
                      interval_hours = (latest_time - previous_time).total_seconds() / 3600
                  else:
                      interval_hours = 0
                  
                  # Quality assessment
                  quality_issues = []
                  quality_score = 100
                  
                  if success_rate < 70:
                      quality_issues.append(f"Low success rate: {success_rate:.1f}%")
                      quality_score -= 30
                  
                  if consecutive_failures >= 3:
                      quality_issues.append(f"Consecutive failures: {consecutive_failures}")
                      quality_score -= 25
                  
                  if interval_hours > 2:  # Should run every 30 minutes
                      quality_issues.append(f"Training interval too long: {interval_hours:.1f}h")
                      quality_score -= 20
                  
                  # Check model freshness
                  latest_success = None
                  for run in runs:
                      if run['conclusion'] == 'success':
                          latest_success = run
                          break
                  
                  if latest_success:
                      latest_time = datetime.datetime.fromisoformat(latest_success['created_at'].replace('Z', '+00:00'))
                      hours_since_success = (datetime.datetime.now(datetime.timezone.utc) - latest_time).total_seconds() / 3600
                      
                      if hours_since_success > 6:
                          quality_issues.append(f"Models stale: {hours_since_success:.1f}h since last success")
                          quality_score -= 15
                  else:
                      quality_issues.append("No successful runs found")
                      quality_score -= 50
                  
                  quality_score = max(0, quality_score)
                  
                  return {
                      'quality_score': quality_score,
                      'success_rate': success_rate,
                      'consecutive_failures': consecutive_failures,
                      'interval_hours': interval_hours,
                      'quality_issues': quality_issues,
                      'total_runs': total_runs,
                      'successful_runs': successful_runs,
                      'failed_runs': failed_runs,
                      'assessment_time': datetime.datetime.utcnow().isoformat()
                  }
                  
              except Exception as e:
                  return {'error': f'Quality check failed: {str(e)}'}
          
          def check_model_metrics():
              """Check model quality metrics (simulated)"""
              # In production, this would download and validate actual models
              # For now, we'll simulate based on workflow success
              
              try:
                  # This would normally download models from S3 and validate them
                  # aws s3 cp s3://bucket/models/current.json current.json
                  
                  # Simulated model metrics
                  model_metrics = {
                      'meta_model_accuracy': 0.732,
                      'execution_model_precision': 0.685,
                      'rl_sizer_performance': 0.821,
                      'feature_count': 17,
                      'training_samples': 15000,
                      'validation_loss': 0.023,
                      'overfitting_score': 0.15,  # Lower is better
                      'model_size_mb': 2.3
                  }
                  
                  # Quality thresholds
                  quality_checks = {
                      'meta_accuracy': model_metrics['meta_model_accuracy'] >= 0.70,
                      'execution_precision': model_metrics['execution_model_precision'] >= 0.65,
                      'rl_performance': model_metrics['rl_sizer_performance'] >= 0.75,
                      'sufficient_samples': model_metrics['training_samples'] >= 10000,
                      'low_overfitting': model_metrics['overfitting_score'] <= 0.20,
                      'reasonable_size': model_metrics['model_size_mb'] <= 10.0
                  }
                  
                  passed_checks = sum(quality_checks.values())
                  total_checks = len(quality_checks)
                  model_quality_score = (passed_checks / total_checks) * 100
                  
                  return {
                      'model_quality_score': model_quality_score,
                      'model_metrics': model_metrics,
                      'quality_checks': quality_checks,
                      'passed_checks': passed_checks,
                      'total_checks': total_checks
                  }
                  
              except Exception as e:
                  return {'error': f'Model metrics check failed: {str(e)}'}
          
          def generate_quality_report():
              """Generate comprehensive quality report"""
              workflow_quality = check_workflow_quality()
              model_quality = check_model_metrics()
              
              # Overall quality assessment
              if 'error' in workflow_quality:
                  overall_score = 0
                  overall_status = 'ERROR'
              elif 'error' in model_quality:
                  overall_score = workflow_quality.get('quality_score', 0) * 0.7  # Workflow only
                  overall_status = 'DEGRADED'
              else:
                  workflow_score = workflow_quality.get('quality_score', 0)
                  model_score = model_quality.get('model_quality_score', 0)
                  overall_score = (workflow_score * 0.6) + (model_score * 0.4)
                  
                  if overall_score >= 80:
                      overall_status = 'EXCELLENT'
                  elif overall_score >= 60:
                      overall_status = 'GOOD'
                  elif overall_score >= 40:
                      overall_status = 'WARNING'
                  else:
                      overall_status = 'CRITICAL'
              
              report = {
                  'overall_score': round(overall_score, 1),
                  'overall_status': overall_status,
                  'workflow_quality': workflow_quality,
                  'model_quality': model_quality,
                  'timestamp': datetime.datetime.utcnow().isoformat(),
                  'report_version': '1.0'
              }
              
              return report
          
          def save_quality_report():
              """Save quality report and trigger alerts if needed"""
              report = generate_quality_report()
              
              os.makedirs('quality-reports', exist_ok=True)
              
              # Save detailed report
              with open('quality-reports/latest.json', 'w') as f:
                  json.dump(report, f, indent=2)
              
              # Save with timestamp for history
              timestamp = datetime.datetime.utcnow().strftime('%Y%m%d_%H%M%S')
              with open(f'quality-reports/report_{timestamp}.json', 'w') as f:
                  json.dump(report, f, indent=2)
              
              # Print summary
              print("üîç Quality Assessment Complete")
              print("==============================")
              print(f"Overall Score: {report['overall_score']}/100")
              print(f"Status: {report['overall_status']}")
              
              workflow_q = report.get('workflow_quality', {})
              if 'error' not in workflow_q:
                  print(f"Success Rate: {workflow_q.get('success_rate', 0):.1f}%")
                  print(f"Quality Issues: {len(workflow_q.get('quality_issues', []))}")
              
              model_q = report.get('model_quality', {})
              if 'error' not in model_q:
                  print(f"Model Quality: {model_q.get('model_quality_score', 0):.1f}/100")
              
              # Alert conditions
              alert_needed = False
              alert_reasons = []
              
              if report['overall_score'] < 40:
                  alert_needed = True
                  alert_reasons.append("Overall quality critical")
              
              if workflow_q.get('consecutive_failures', 0) >= 3:
                  alert_needed = True
                  alert_reasons.append("Multiple consecutive training failures")
              
              if workflow_q.get('success_rate', 100) < 50:
                  alert_needed = True
                  alert_reasons.append("Training success rate below 50%")
              
              if alert_needed:
                  print("\nüö® QUALITY ALERT TRIGGERED")
                  print("Reasons:")
                  for reason in alert_reasons:
                      print(f"  - {reason}")
              else:
                  print("\n‚úÖ Quality within acceptable parameters")
              
              return report
          
          if __name__ == "__main__":
              save_quality_report()
          EOF
          
          # Run quality assessment
          python quality_monitor.py
          
      - name: "‚òÅÔ∏è Upload Quality Reports"
        if: env.AWS_ACCESS_KEY_ID != ''
        run: |
          # Upload quality reports to S3
          aws s3 sync quality-reports/ "s3://${{ secrets.S3_BUCKET }}/quality/" \
            --acl public-read \
            --cache-control "max-age=300" \
            --exclude "*" --include "*.json" || echo "S3 upload optional"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          
      - name: "üìß Quality Notifications"
        if: env.GITHUB_TOKEN != ''
        run: |
          # Create notification script for quality issues
          cat > send_notifications.py << 'EOF'
          import json
          import requests
          import os
          
          def check_quality_alerts():
              """Check if quality alerts need to be sent"""
              try:
                  with open('quality-reports/latest.json', 'r') as f:
                      report = json.load(f)
                  
                  overall_score = report.get('overall_score', 100)
                  overall_status = report.get('overall_status', 'UNKNOWN')
                  
                  # Determine if we need to send an alert
                  alert_needed = overall_score < 40 or overall_status in ['CRITICAL', 'ERROR']
                  
                  if alert_needed:
                      send_github_issue_comment(report)
                  
                  return alert_needed
                  
              except Exception as e:
                  print(f"Error checking quality alerts: {e}")
                  return False
          
          def send_github_issue_comment(report):
              """Send quality alert as GitHub issue comment (if issue #1 exists)"""
              try:
                  headers = {
                      'Accept': 'application/vnd.github.v3+json',
                      'Authorization': f'token ${{ secrets.GITHUB_TOKEN }}'
                  }
                  
                  repo = '${{ github.repository }}'
                  
                  # Create alert message
                  workflow_q = report.get('workflow_quality', {})
                  quality_issues = workflow_q.get('quality_issues', [])
                  
                  message = f"""üö® **Quality Alert** - {report['overall_status']}
          
          **Overall Score**: {report['overall_score']}/100
          **Timestamp**: {report['timestamp']}
          
          **Issues Detected**:
          """
                  
                  for issue in quality_issues:
                      message += f"- {issue}\n"
                  
                  message += f"""
          **Quick Actions**:
          - Check [GitHub Actions](https://github.com/{repo}/actions) for failed workflows
          - Review [Training Logs](https://github.com/{repo}/actions/workflows/train-continuous-clean.yml)
          - Monitor [Dashboard](http://localhost:5050/dashboard) for real-time status
          
          *This alert was automatically generated by the Quality Assurance Monitor.*
          """
                  
                  # Try to post to issue #1 (monitoring issue)
                  url = f'https://api.github.com/repos/{repo}/issues/1/comments'
                  data = {'body': message}
                  
                  response = requests.post(url, headers=headers, json=data)
                  
                  if response.status_code == 201:
                      print("‚úÖ Quality alert posted to GitHub issue")
                  else:
                      print(f"Failed to post GitHub comment: {response.status_code}")
                      print("Alert message:")
                      print(message)
                  
              except Exception as e:
                  print(f"Error sending GitHub notification: {e}")
          
          if __name__ == "__main__":
              alert_sent = check_quality_alerts()
              if alert_sent:
                  print("üö® Quality alert triggered and sent")
              else:
                  print("‚úÖ No quality alerts needed")
          EOF
          
          # Check and send notifications
          python send_notifications.py
          
      - name: "üìã Quality Summary"
        run: |
          echo "üîç Quality Assurance Monitor Complete"
          echo "===================================="
          
          if [ -f "quality-reports/latest.json" ]; then
              echo "‚úÖ Quality report generated"
              
              # Extract key metrics
              python << 'EOF'
          import json
          
          with open('quality-reports/latest.json', 'r') as f:
              report = json.load(f)
          
          print(f"Overall Score: {report['overall_score']}/100")
          print(f"Status: {report['overall_status']}")
          
          workflow_q = report.get('workflow_quality', {})
          if 'error' not in workflow_q:
              print(f"Training Success Rate: {workflow_q.get('success_rate', 0):.1f}%")
              print(f"Consecutive Failures: {workflow_q.get('consecutive_failures', 0)}")
              print(f"Quality Issues: {len(workflow_q.get('quality_issues', []))}")
          
          model_q = report.get('model_quality', {})
          if 'error' not in model_q:
              print(f"Model Quality Score: {model_q.get('model_quality_score', 0):.1f}/100")
          EOF
          
          else
              echo "‚ùå No quality report generated"
          fi
          
          echo ""
          echo "üìä Quality Monitoring Active"
          echo "Next assessment: $(date -d '+2 hours' '+%Y-%m-%d %H:%M UTC')"