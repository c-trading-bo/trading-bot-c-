name: Enhanced 24/7 Data Collection - Cloud + Local Architecture

on:
  schedule:
    # Enhanced collection every 5 minutes during market hours (14:30-21:00 UTC = 9:30-4:00 EST)
    - cron: '*/5 14-20 * * 1-5'
    # Macro data every 15 minutes (less frequent as it changes slower)  
    - cron: '*/15 * * * *'
    # News and sentiment every 10 minutes
    - cron: '*/10 * * * *'
  workflow_dispatch:
    inputs:
      force_full_collection:
        description: 'Force full data collection (options + macro + sentiment)'
        required: false
        default: 'false'

env:
  PYTHONUNBUFFERED: 1
  TZ: America/New_York

jobs:
  # ============================================
  # JOB 1: ENHANCED OPTIONS & GAMMA ANALYSIS
  # ============================================
  collect-options-gamma:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - uses: actions/checkout@v3
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install Dependencies
      run: |
        pip install yfinance pandas numpy scipy requests beautifulsoup4 ta pytrends fredapi
        pip install matplotlib plotly seaborn scikit-learn
    
    - name: Enhanced Options Flow Collection
      run: |
        python << 'EOF'
        import yfinance as yf
        import pandas as pd
        import numpy as np
        import json
        import os
        from datetime import datetime, timedelta
        
        # Enhanced options data collection for SPY, QQQ, IWM, DIA, VIX
        symbols = ['SPY', 'QQQ', 'IWM', 'DIA', 'VIX', 'GLD', 'TLT']
        
        def collect_enhanced_options_data():
            data_dir = "Intelligence/data/options"
            os.makedirs(f"{data_dir}/flow", exist_ok=True)
            os.makedirs(f"{data_dir}/gamma", exist_ok=True)
            
            results = {
                'timestamp': datetime.now().isoformat(),
                'symbols': {},
                'market_summary': {}
            }
            
            total_call_volume = 0
            total_put_volume = 0
            total_call_oi = 0
            total_put_oi = 0
            
            for symbol in symbols:
                try:
                    ticker = yf.Ticker(symbol)
                    hist = ticker.history(period="2d")
                    
                    if hist.empty:
                        continue
                        
                    current_price = hist['Close'].iloc[-1]
                    prev_close = hist['Close'].iloc[-2] if len(hist) > 1 else current_price
                    price_change_pct = ((current_price - prev_close) / prev_close) * 100
                    
                    # Get options chain
                    expirations = ticker.options
                    if not expirations:
                        continue
                    
                    symbol_data = {
                        'current_price': float(current_price),
                        'price_change_pct': float(price_change_pct),
                        'expirations': [],
                        'summary': {}
                    }
                    
                    # Analyze multiple expirations for better gamma exposure calculation
                    for i, exp_date in enumerate(expirations[:3]):  # First 3 expirations
                        try:
                            option_chain = ticker.option_chain(exp_date)
                            calls = option_chain.calls
                            puts = option_chain.puts
                            
                            if calls.empty or puts.empty:
                                continue
                            
                            # Enhanced gamma calculation
                            def calculate_gamma_exposure(options_df, option_type, spot_price):
                                gamma_exposure = 0
                                for _, option in options_df.iterrows():
                                    strike = option['strike']
                                    volume = option.get('volume', 0) or 0
                                    open_interest = option.get('openInterest', 0) or 0
                                    
                                    # Simple Black-Scholes gamma approximation
                                    # Gamma is highest near ATM strikes
                                    moneyness = abs(strike - spot_price) / spot_price
                                    gamma_factor = max(0, 1 - moneyness * 5)  # Peaks at ATM, decreases away
                                    
                                    if option_type == 'call':
                                        gamma_exposure += volume * 100 * gamma_factor  # 100 shares per contract
                                    else:  # put
                                        gamma_exposure -= volume * 100 * gamma_factor  # Negative for puts
                                
                                return gamma_exposure
                            
                            call_gamma = calculate_gamma_exposure(calls, 'call', current_price)
                            put_gamma = calculate_gamma_exposure(puts, 'put', current_price)
                            net_gamma = call_gamma + put_gamma
                            
                            # Calculate volume and OI metrics
                            call_volume = calls['volume'].fillna(0).sum()
                            put_volume = puts['volume'].fillna(0).sum()
                            call_oi = calls['openInterest'].fillna(0).sum()
                            put_oi = puts['openInterest'].fillna(0).sum()
                            
                            total_call_volume += call_volume
                            total_put_volume += put_volume
                            total_call_oi += call_oi
                            total_put_oi += put_oi
                            
                            # Find max pain (strike with highest total OI)
                            calls['total_oi'] = calls['openInterest'].fillna(0)
                            puts['total_oi'] = puts['openInterest'].fillna(0)
                            
                            combined_strikes = pd.concat([
                                calls[['strike', 'total_oi']].rename(columns={'total_oi': 'call_oi'}),
                                puts[['strike', 'total_oi']].rename(columns={'total_oi': 'put_oi'})
                            ]).groupby('strike').sum().fillna(0)
                            
                            combined_strikes['total_oi'] = combined_strikes['call_oi'] + combined_strikes['put_oi']
                            max_pain_strike = combined_strikes['total_oi'].idxmax() if not combined_strikes.empty else current_price
                            
                            exp_data = {
                                'expiration': exp_date,
                                'days_to_exp': (pd.to_datetime(exp_date) - pd.Timestamp.now()).days,
                                'call_volume': int(call_volume),
                                'put_volume': int(put_volume),
                                'call_oi': int(call_oi),
                                'put_oi': int(put_oi),
                                'put_call_ratio': float(put_volume / call_volume) if call_volume > 0 else 0,
                                'gamma_exposure': {
                                    'call_gamma': float(call_gamma),
                                    'put_gamma': float(put_gamma),
                                    'net_gamma': float(net_gamma)
                                },
                                'max_pain': float(max_pain_strike)
                            }
                            
                            symbol_data['expirations'].append(exp_data)
                            
                        except Exception as e:
                            print(f"Error processing {symbol} expiration {exp_date}: {e}")
                            continue
                    
                    # Symbol summary
                    if symbol_data['expirations']:
                        total_call_vol = sum(exp['call_volume'] for exp in symbol_data['expirations'])
                        total_put_vol = sum(exp['put_volume'] for exp in symbol_data['expirations'])
                        avg_put_call_ratio = np.mean([exp['put_call_ratio'] for exp in symbol_data['expirations']])
                        total_net_gamma = sum(exp['gamma_exposure']['net_gamma'] for exp in symbol_data['expirations'])
                        
                        symbol_data['summary'] = {
                            'total_call_volume': total_call_vol,
                            'total_put_volume': total_put_vol,
                            'avg_put_call_ratio': float(avg_put_call_ratio),
                            'total_net_gamma_exposure': float(total_net_gamma),
                            'gamma_flip_level': float(current_price + (total_net_gamma / 10000))  # Rough estimate
                        }
                    
                    results['symbols'][symbol] = symbol_data
                    print(f"‚úÖ Collected enhanced options data for {symbol}")
                    
                except Exception as e:
                    print(f"‚ùå Error collecting options data for {symbol}: {e}")
                    continue
            
            # Market-wide summary
            market_pcr = total_put_volume / total_call_volume if total_call_volume > 0 else 0
            results['market_summary'] = {
                'total_call_volume': int(total_call_volume),
                'total_put_volume': int(total_put_volume),
                'total_call_oi': int(total_call_oi),
                'total_put_oi': int(total_put_oi),
                'market_put_call_ratio': float(market_pcr),
                'collection_time': datetime.now().isoformat()
            }
            
            # Save enhanced options data
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            with open(f"{data_dir}/flow/enhanced_options_{timestamp}.json", 'w') as f:
                json.dump(results, f, indent=2)
            
            # Save latest as well for easy access
            with open(f"{data_dir}/flow/latest_enhanced_options.json", 'w') as f:
                json.dump(results, f, indent=2)
            
            print(f"üéâ Enhanced options data collection complete!")
            print(f"üìä Market P/C Ratio: {market_pcr:.3f}")
            print(f"üìà Total Call Volume: {total_call_volume:,}")
            print(f"üìâ Total Put Volume: {total_put_volume:,}")
            
            return results
        
        # Execute enhanced collection
        collect_enhanced_options_data()
        EOF

  # ============================================
  # JOB 2: MACRO DATA COLLECTION (FRED + MORE)
  # ============================================
  collect-macro-data:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - uses: actions/checkout@v3
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install Dependencies
      run: |
        pip install pandas numpy requests fredapi yfinance
        pip install beautifulsoup4 lxml
    
    - name: Collect Comprehensive Macro Data
      env:
        FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
      run: |
        python << 'EOF'
        import pandas as pd
        import numpy as np
        import requests
        import json
        import os
        import yfinance as yf
        from datetime import datetime, timedelta
        try:
            from fredapi import Fred
            HAS_FRED = True
        except ImportError:
            HAS_FRED = False
            print("‚ö†Ô∏è FRED API not available, using alternative sources")
        
        def collect_macro_data():
            data_dir = "Intelligence/data/macro"
            os.makedirs(data_dir, exist_ok=True)
            
            results = {
                'timestamp': datetime.now().isoformat(),
                'treasury_yields': {},
                'economic_indicators': {},
                'currencies': {},
                'commodities': {},
                'volatility_indices': {},
                'sentiment_indicators': {}
            }
            
            # 1. Treasury Yields via Yahoo Finance (most reliable)
            print("üìà Collecting Treasury Yields...")
            try:
                yield_symbols = {
                    '^IRX': '3_month',     # 3-Month Treasury
                    '^FVX': '5_year',      # 5-Year Treasury  
                    '^TNX': '10_year',     # 10-Year Treasury
                    '^TYX': '30_year'      # 30-Year Treasury
                }
                
                for symbol, name in yield_symbols.items():
                    try:
                        ticker = yf.Ticker(symbol)
                        hist = ticker.history(period="5d")
                        if not hist.empty:
                            current_yield = hist['Close'].iloc[-1]
                            prev_yield = hist['Close'].iloc[-2] if len(hist) > 1 else current_yield
                            change = current_yield - prev_yield
                            
                            results['treasury_yields'][name] = {
                                'current': float(current_yield),
                                'previous': float(prev_yield),
                                'change': float(change),
                                'change_bps': float(change * 100)  # Basis points
                            }
                            print(f"  ‚úÖ {name}: {current_yield:.3f}% ({change:+.1f} bps)")
                    except Exception as e:
                        print(f"  ‚ùå Error getting {name}: {e}")
                        
            except Exception as e:
                print(f"‚ùå Error collecting Treasury yields: {e}")
            
            # 2. Major Currency Pairs
            print("üí± Collecting Currency Data...")
            try:
                currency_symbols = {
                    'DX-Y.NYB': 'dxy',           # Dollar Index
                    'EURUSD=X': 'eur_usd',       # EUR/USD
                    'GBPUSD=X': 'gbp_usd',       # GBP/USD
                    'USDJPY=X': 'usd_jpy',       # USD/JPY
                    'AUDUSD=X': 'aud_usd',       # AUD/USD
                    'USDCAD=X': 'usd_cad'        # USD/CAD
                }
                
                for symbol, name in currency_symbols.items():
                    try:
                        ticker = yf.Ticker(symbol)
                        hist = ticker.history(period="5d")
                        if not hist.empty:
                            current_rate = hist['Close'].iloc[-1]
                            prev_rate = hist['Close'].iloc[-2] if len(hist) > 1 else current_rate
                            change_pct = ((current_rate - prev_rate) / prev_rate) * 100
                            
                            results['currencies'][name] = {
                                'current': float(current_rate),
                                'previous': float(prev_rate),
                                'change_pct': float(change_pct)
                            }
                            print(f"  ‚úÖ {name}: {current_rate:.4f} ({change_pct:+.2f}%)")
                    except Exception as e:
                        print(f"  ‚ùå Error getting {name}: {e}")
                        
            except Exception as e:
                print(f"‚ùå Error collecting currency data: {e}")
            
            # 3. Key Commodities
            print("üõ¢Ô∏è Collecting Commodity Data...")
            try:
                commodity_symbols = {
                    'CL=F': 'crude_oil',         # Crude Oil
                    'GC=F': 'gold',              # Gold
                    'SI=F': 'silver',            # Silver
                    'NG=F': 'natural_gas',       # Natural Gas
                    'ZC=F': 'corn',              # Corn
                    'ZW=F': 'wheat'              # Wheat
                }
                
                for symbol, name in commodity_symbols.items():
                    try:
                        ticker = yf.Ticker(symbol)
                        hist = ticker.history(period="5d")
                        if not hist.empty:
                            current_price = hist['Close'].iloc[-1]
                            prev_price = hist['Close'].iloc[-2] if len(hist) > 1 else current_price
                            change_pct = ((current_price - prev_price) / prev_price) * 100
                            
                            results['commodities'][name] = {
                                'current': float(current_price),
                                'previous': float(prev_price),
                                'change_pct': float(change_pct)
                            }
                            print(f"  ‚úÖ {name}: ${current_price:.2f} ({change_pct:+.2f}%)")
                    except Exception as e:
                        print(f"  ‚ùå Error getting {name}: {e}")
                        
            except Exception as e:
                print(f"‚ùå Error collecting commodity data: {e}")
            
            # 4. Volatility Indices
            print("üìä Collecting Volatility Indices...")
            try:
                vol_symbols = {
                    '^VIX': 'vix',               # CBOE Volatility Index
                    '^VXN': 'vxn',               # NASDAQ Volatility Index
                    '^RVX': 'rvx',               # Russell 2000 Volatility Index
                    '^VXD': 'vxd'                # DJIA Volatility Index
                }
                
                for symbol, name in vol_symbols.items():
                    try:
                        ticker = yf.Ticker(symbol)
                        hist = ticker.history(period="5d")
                        if not hist.empty:
                            current_vol = hist['Close'].iloc[-1]
                            prev_vol = hist['Close'].iloc[-2] if len(hist) > 1 else current_vol
                            change = current_vol - prev_vol
                            change_pct = ((current_vol - prev_vol) / prev_vol) * 100
                            
                            results['volatility_indices'][name] = {
                                'current': float(current_vol),
                                'previous': float(prev_vol),
                                'change': float(change),
                                'change_pct': float(change_pct)
                            }
                            print(f"  ‚úÖ {name}: {current_vol:.2f} ({change:+.2f})")
                    except Exception as e:
                        print(f"  ‚ùå Error getting {name}: {e}")
                        
            except Exception as e:
                print(f"‚ùå Error collecting volatility data: {e}")
            
            # 5. FRED Economic Data (if available)
            if HAS_FRED and os.getenv('FRED_API_KEY'):
                print("üè¶ Collecting FRED Economic Data...")
                try:
                    fred = Fred(api_key=os.getenv('FRED_API_KEY'))
                    
                    # Key economic indicators
                    fred_series = {
                        'FEDFUNDS': 'fed_funds_rate',
                        'UNRATE': 'unemployment_rate',
                        'CPIAUCSL': 'cpi',
                        'GDP': 'gdp',
                        'DEXUSEU': 'usd_eur_rate',
                        'T10Y2Y': 'yield_curve_10y2y',
                        'DTWEXBGS': 'trade_weighted_dollar'
                    }
                    
                    for series_id, name in fred_series.items():
                        try:
                            # Get latest data point
                            data = fred.get_series(series_id, limit=2)
                            if not data.empty:
                                current_value = data.iloc[-1]
                                prev_value = data.iloc[-2] if len(data) > 1 else current_value
                                
                                results['economic_indicators'][name] = {
                                    'current': float(current_value) if pd.notna(current_value) else None,
                                    'previous': float(prev_value) if pd.notna(prev_value) else None,
                                    'last_updated': data.index[-1].isoformat(),
                                    'series_id': series_id
                                }
                                print(f"  ‚úÖ {name}: {current_value}")
                        except Exception as e:
                            print(f"  ‚ö†Ô∏è Could not get {name}: {e}")
                            
                except Exception as e:
                    print(f"‚ùå Error with FRED API: {e}")
            else:
                print("‚ö†Ô∏è FRED API key not available, skipping economic indicators")
            
            # 6. Market Sentiment Indicators (Alternative data)
            print("üé≠ Collecting Sentiment Indicators...")
            try:
                # Fear & Greed Index alternative calculation
                # Based on VIX, Put/Call ratio, Market momentum, Safe haven demand
                vix_value = results.get('volatility_indices', {}).get('vix', {}).get('current', 20)
                
                # Simple Fear & Greed calculation
                # VIX: Lower = Greed, Higher = Fear
                fear_greed_score = max(0, min(100, 100 - (vix_value - 10) * 4))
                
                sentiment_label = "Extreme Fear"
                if fear_greed_score > 75:
                    sentiment_label = "Extreme Greed"
                elif fear_greed_score > 55:
                    sentiment_label = "Greed"
                elif fear_greed_score > 45:
                    sentiment_label = "Neutral"
                elif fear_greed_score > 25:
                    sentiment_label = "Fear"
                
                results['sentiment_indicators']['fear_greed_index'] = {
                    'score': float(fear_greed_score),
                    'label': sentiment_label,
                    'calculation_basis': 'VIX-based approximation'
                }
                
                print(f"  ‚úÖ Fear & Greed Index: {fear_greed_score:.1f} ({sentiment_label})")
                
            except Exception as e:
                print(f"‚ùå Error calculating sentiment: {e}")
            
            # Save macro data
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            with open(f"{data_dir}/macro_data_{timestamp}.json", 'w') as f:
                json.dump(results, f, indent=2)
            
            # Save latest
            with open(f"{data_dir}/latest_macro_data.json", 'w') as f:
                json.dump(results, f, indent=2)
            
            print(f"\nüéâ Comprehensive macro data collection complete!")
            print(f"üìä Data points collected: {len(results['treasury_yields']) + len(results['currencies']) + len(results['commodities']) + len(results['volatility_indices'])}")
            
            return results
        
        # Execute comprehensive macro collection
        collect_macro_data()
        EOF

  # ============================================  
  # JOB 3: NEWS & SENTIMENT COLLECTION
  # ============================================
  collect-news-sentiment:
    runs-on: ubuntu-latest
    timeout-minutes: 12
    
    steps:
    - uses: actions/checkout@v3
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install Dependencies
      run: |
        pip install requests beautifulsoup4 pandas numpy
        pip install feedparser textblob pytrends
    
    - name: Enhanced News & Sentiment Collection
      run: |
        python << 'EOF'
        import requests
        import json
        import os
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        import re
        from urllib.parse import urljoin
        try:
            from textblob import TextBlob
            from pytrends.request import TrendReq
            HAS_SENTIMENT = True
        except ImportError:
            HAS_SENTIMENT = False
            print("‚ö†Ô∏è Sentiment analysis libraries not available")
        
        def collect_enhanced_news_sentiment():
            data_dir = "Intelligence/data/news"
            os.makedirs(data_dir, exist_ok=True)
            
            results = {
                'timestamp': datetime.now().isoformat(),
                'market_news': [],
                'economic_calendar': [],
                'sentiment_analysis': {},
                'trending_topics': {},
                'news_summary': {}
            }
            
            # 1. Financial News Sources
            print("üì∞ Collecting Financial News...")
            
            news_sources = [
                {
                    'name': 'MarketWatch',
                    'url': 'https://feeds.content.dowjones.io/public/rss/mw_realtimeheadlines',
                    'type': 'rss'
                },
                {
                    'name': 'Yahoo Finance',
                    'url': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
                    'type': 'rss'
                }
            ]
            
            all_headlines = []
            
            for source in news_sources:
                try:
                    if source['type'] == 'rss':
                        # Simple RSS parsing without feedparser for now
                        response = requests.get(source['url'], timeout=10)
                        if response.status_code == 200:
                            # Extract titles using regex (basic RSS parsing)
                            titles = re.findall(r'<title[^>]*>(.*?)</title>', response.text)
                            descriptions = re.findall(r'<description[^>]*>(.*?)</description>', response.text)
                            
                            for i, title in enumerate(titles[1:6]):  # Skip first title (usually feed title), take 5
                                headline = {
                                    'source': source['name'],
                                    'title': title.strip(),
                                    'description': descriptions[i+1].strip() if i+1 < len(descriptions) else '',
                                    'timestamp': datetime.now().isoformat()
                                }
                                
                                # Basic sentiment analysis if available
                                if HAS_SENTIMENT:
                                    blob = TextBlob(title)
                                    headline['sentiment'] = {
                                        'polarity': blob.sentiment.polarity,  # -1 to 1
                                        'subjectivity': blob.sentiment.subjectivity  # 0 to 1
                                    }
                                
                                all_headlines.append(headline)
                                results['market_news'].append(headline)
                            
                            print(f"  ‚úÖ {source['name']}: {len(titles[1:6])} headlines")
                        
                except Exception as e:
                    print(f"  ‚ùå Error collecting from {source['name']}: {e}")
            
            # 2. Key Economic Events (Hardcoded calendar - could be enhanced with API)
            print("üìÖ Adding Economic Calendar Events...")
            try:
                # Major economic events this week (would normally come from economic calendar API)
                economic_events = [
                    {
                        'event': 'FOMC Meeting Decision',
                        'importance': 'high',
                        'currency': 'USD',
                        'expected_impact': 'high_volatility'
                    },
                    {
                        'event': 'Non-Farm Payrolls',
                        'importance': 'high', 
                        'currency': 'USD',
                        'expected_impact': 'high_volatility'
                    },
                    {
                        'event': 'CPI Data Release',
                        'importance': 'high',
                        'currency': 'USD', 
                        'expected_impact': 'medium_volatility'
                    }
                ]
                
                results['economic_calendar'] = economic_events
                print(f"  ‚úÖ Added {len(economic_events)} economic events")
                
            except Exception as e:
                print(f"‚ùå Error adding economic events: {e}")
            
            # 3. Sentiment Analysis Summary
            if HAS_SENTIMENT and all_headlines:
                print("üé≠ Analyzing Overall Market Sentiment...")
                try:
                    sentiments = [h.get('sentiment', {}) for h in all_headlines if 'sentiment' in h]
                    
                    if sentiments:
                        avg_polarity = np.mean([s.get('polarity', 0) for s in sentiments])
                        avg_subjectivity = np.mean([s.get('subjectivity', 0.5) for s in sentiments])
                        
                        sentiment_label = "Neutral"
                        if avg_polarity > 0.1:
                            sentiment_label = "Positive" if avg_polarity < 0.5 else "Very Positive"
                        elif avg_polarity < -0.1:
                            sentiment_label = "Negative" if avg_polarity > -0.5 else "Very Negative"
                        
                        results['sentiment_analysis'] = {
                            'overall_polarity': float(avg_polarity),
                            'overall_subjectivity': float(avg_subjectivity),
                            'sentiment_label': sentiment_label,
                            'confidence': float(1 - avg_subjectivity),  # Lower subjectivity = higher confidence
                            'headlines_analyzed': len(sentiments)
                        }
                        
                        print(f"  ‚úÖ Overall Sentiment: {sentiment_label} (polarity: {avg_polarity:.3f})")
                
                except Exception as e:
                    print(f"‚ùå Error in sentiment analysis: {e}")
            
            # 4. Market Keywords & Trends
            print("üîç Analyzing Market Keywords...")
            try:
                # Extract common market keywords from headlines
                market_keywords = [
                    'fed', 'federal reserve', 'interest rate', 'inflation', 'cpi', 
                    'unemployment', 'jobs', 'gdp', 'recession', 'bull', 'bear',
                    'earnings', 'revenue', 'guidance', 'fomc', 'powell', 'yellen'
                ]
                
                keyword_mentions = {}
                all_text = ' '.join([h['title'].lower() for h in all_headlines])
                
                for keyword in market_keywords:
                    count = all_text.count(keyword.lower())
                    if count > 0:
                        keyword_mentions[keyword] = count
                
                # Sort by frequency
                trending_keywords = dict(sorted(keyword_mentions.items(), key=lambda x: x[1], reverse=True)[:10])
                results['trending_topics']['keywords'] = trending_keywords
                
                print(f"  ‚úÖ Top trending keywords: {list(trending_keywords.keys())[:5]}")
                
            except Exception as e:
                print(f"‚ùå Error analyzing keywords: {e}")
            
            # 5. News Summary
            results['news_summary'] = {
                'total_headlines': len(all_headlines),
                'sources_count': len(news_sources),
                'collection_time': datetime.now().isoformat(),
                'has_sentiment_analysis': HAS_SENTIMENT,
                'trending_keywords_count': len(results.get('trending_topics', {}).get('keywords', {}))
            }
            
            # Save news data
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            with open(f"{data_dir}/news_sentiment_{timestamp}.json", 'w') as f:
                json.dump(results, f, indent=2)
            
            # Save latest
            with open(f"{data_dir}/latest_news_sentiment.json", 'w') as f:
                json.dump(results, f, indent=2)
            
            print(f"\nüéâ Enhanced news & sentiment collection complete!")
            print(f"üì∞ Headlines collected: {len(all_headlines)}")
            print(f"üé≠ Sentiment: {results.get('sentiment_analysis', {}).get('sentiment_label', 'N/A')}")
            
            return results
        
        # Execute enhanced news collection
        collect_enhanced_news_sentiment()
        EOF

  # ============================================
  # JOB 4: COMMIT ALL COLLECTED DATA
  # ============================================  
  commit-data:
    runs-on: ubuntu-latest
    needs: [collect-options-gamma, collect-macro-data, collect-news-sentiment]
    if: always()
    
    steps:
    - uses: actions/checkout@v3
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: Download all artifacts and commit
      run: |
        # Configure git
        git config user.name "GitHub Actions Enhanced Data Collection"
        git config user.email "actions@github.com"
        
        # Add all Intelligence data
        git add Intelligence/data/
        
        # Check if there are changes to commit
        if git diff --quiet --cached; then
          echo "üìù No new data to commit"
        else
          # Commit with detailed message
          TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S UTC')
          git commit -m "üöÄ Enhanced 24/7 Data Collection - ${TIMESTAMP}
          
          ‚úÖ Options & Gamma Analysis
          ‚úÖ Comprehensive Macro Data (Yields, Currencies, Commodities)  
          ‚úÖ News & Sentiment Analysis
          ‚úÖ Economic Calendar Integration
          
          üìä Cloud-based collection complete - Ready for local bot consumption"
          
          git push
          echo "‚úÖ Enhanced data collection committed and pushed!"
        fi